tooltip = "name",
interactive = TRUE ) # 한글 깨짐
gc <- geocode( enc2utf8( "서울시청" ) )
gc
class(gc)
str(gc)
install.packages("wordcloud")
install.packages("wordcloud2")
install.packages("KoNLP")
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(wordcloud)
library(wordcloud2)
library(KoNLP)
library(RColorBrewer)
library(dplyr)
library(ggplot2)
text <- readLines( 'mis_document.txt', encoding = 'UTF-8' )
text
str(text)
class(text)
buildDictionary( ext_dic = 'woorimalsam' )
pal2 <- brewer.pal( 8, 'Dark2' ) # 색상 팔레트 생성
noun <- sapply( text, extractNoun, USE.NAMES = F ) #명사 추출
noun
noun2 <- unlist( noun ) # list -> vector로 변환
wordcount <- table( noun2 )
sort.noun <- sort( wordcount, decreasing = T )[ 1:10 ]
sort.noun
sort.noun <- sort.noun[ -1 ]
barplot( sort.noun, names.arg = names( sort.noun ),
col = 'steelblue', main = '빈도수 높은 단어',
ylab = '단어 빈도수' )
wordcount
sort.noun
noun2 <- unlist( noun ) # list -> vector로 변환
wordcount <- table( noun2 )
sort.noun <- sort( wordcount, decreasing = T )[ 1:10 ]
sort.noun
sort.noun <- sort.noun[ -1 ]
barplot( sort.noun, names.arg = names( sort.noun ),
col = 'steelblue', main = '빈도수 높은 단어',
ylab = '단어 빈도수' )
df <- as.data.frame( sort.noun )
df
ggplot( df, aes( x = df.noun2, y = df$Freq) ) +
geom_bar( stat = 'indentity', width = 0.7, fill = 'steelblue' ) +
ggtitle( '빈도수 높은 단어' ) +
theme( plot.title = element_text( size = 25, face = 'bold', colour = 'steelblue',
hjust = 0, vjust = 1 ) ) +
labs( x = '명사', y = '단어빈도수' ) +
geom_text( aes( label = df$Freq ), hjust = -0.3 ) +
coord_flip()
ggplot( df, aes( x = df.noun2, y = df$Freq) ) +
geom_bar( stat = 'identity', width = 0.7, fill = 'steelblue' ) +
ggtitle( '빈도수 높은 단어' ) +
theme( plot.title = element_text( size = 25, face = 'bold', colour = 'steelblue',
hjust = 0, vjust = 1 ) ) +
labs( x = '명사', y = '단어빈도수' ) +
geom_text( aes( label = df$Freq ), hjust = -0.3 ) +
coord_flip()
ggplot( df, aes( x = df$noun2, y = df$Freq) ) +
geom_bar( stat = 'identity', width = 0.7, fill = 'steelblue' ) +
ggtitle( '빈도수 높은 단어' ) +
theme( plot.title = element_text( size = 25, face = 'bold', colour = 'steelblue',
hjust = 0, vjust = 1 ) ) +
labs( x = '명사', y = '단어빈도수' ) +
geom_text( aes( label = df$Freq ), hjust = -0.3 ) +
coord_flip()
sort.noun
df
wordcloud( names( wordcount ), freq = wordcount, scale = c( 6, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal2 )
wordcloud( names( wordcount ), freq = wordcount, scale = c( 6, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal2 )
wordcloud( names( wordcount ), freq = wordcount, scale = c( 6, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal2 )
wordcloud( names( wordcount ), freq = wordcount, scale = c( 6, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal2 )
warnings()
wordcloud( names( wordcount ), freq = wordcount, scale = c( 4, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal2 )
wordcloud( names( wordcount ), freq = wordcount, scale = c( 4, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal2, res = 300 )
# 5. word cloud 작성
# 인수 순서대로 단어 / 단어빈도 / 단어 폰트 크기 (최대, 최소)
# 단어최소빈도 / 단어출력위치 / 90도 회전단어 비율 / 단어 색
pal3 <- brewer.pal( 9, 'Blues' )[5:9]
wordcloud( names( wordcount ), freq = wordcount, scale = c( 4, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal3 )
pal3 <- brewer.pal( 9, 'Blues' )[5:9]
wordcloud( names( wordcount ), freq = wordcount, scale = c( 4, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal3 )
buildDictionary( ext_dic = 'woorimalsam', user_dic = data.frame( '정치', 'ncn'),
replace_user_dic = T )
noun <- sapply( text, extractNoun, USE.NAMES = F )
noun2 <- unlist( noun )
noun2
noun2 <- noun2[ nchar( noun2 ) > 1 ]
noun2
noun2 <- gsub( '하지', '', noun2 )
noun2 <- gsub( '때문', '', noun2 )
wordcount <- table( noun2 )
wordcount
wordcloud( names( wordcount ), freq = wordcount, scale = c(4, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal2 )
wordcloud( names( wordcount ), freq = wordcount, scale = c(4, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal3 )
#
# 애국가 형태소 분석
#
library( KoNLP )
useSystemDic()
useSystemdic()
useNIADic()
#
# 애국가 가사 다운받기
#
# 1. 사전 설정
useSejongDic()
useSystemDic()
useNIADic()
word_data <- readLines('애국가(가사).txt')
word_data
# 3. 명사 추출
word_data2 <- sapplt( word_data, extractNoun, USE.NAMES = F )
word_data2
word_data2 <- sapply( word_data, extractNoun, USE.NAMES = F )
word_data2
word_data2 <- sapply( word_data, extractNoun, USE.NAMES = F )
word_data2
word_data2
add_words <- c('백두산', '남산', '철갑', '가을', '하늘', '달', '삼천리')
buildDictionary( use_dict = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T )
add_words <- c('백두산', '남산', '철갑', '가을', '하늘', '달', '삼천리')
buildDictionary( use_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T )
# 3.1 제대로 추출되지 않은 단어를 사용자 사전에 등록
add_words <- c('백두산', '남산', '철갑', '가을', '하늘', '달', '삼천리')
buildDictionary( use_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T )
get_dictionary( 'user_dic' )
buildDictionary( user_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T )
get_dictionary( 'user_dic' )
word_data3 <- sapply( word_data, extractNoun, USE.NAMES = F )
word_data3
word_data3
word_data <- readLines('애국가(가사).txt')
word_data
add_words <- c('백두산', '남산', '철갑', '가을', '하늘', '달', '삼천리')
buildDictionary( user_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T )
get_dictionary( 'user_dic' )
word_data3 <- sapply( word_data, extractNoun, USE.NAMES = F )
word_data3
undata <- unlist( word_data3 )
undata
# 5. 사용 빈도 확인
word_table <- table( undata )
word_table
wordcount <- word_table
wordcloud( names( wordcount ), freq = wordcount, scale = c(4, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal3 )
wordcloud( names( wordcount ), freq = wordcount, scale = c(4, 0.7),
min.freq = 1, random.order = F, rot.per = .1, colors = pal3 )
wordcloud( names( wordcount ), freq = wordcount, scale = c(5, 0.7),
min.freq = 1, random.order = F, rot.per = .1, colors = pal3 )
undata2 <- undata[ nchar( undata ) >= 2 ]
undata2
word_table <- table( undata2 )
word_table
undata2 <- undata[ nchar( undata ) >= 2 ]
undata2
word_table2 <- table( undata2 )
word_table2
# 7. 데이터 정렬
sort( word_table2, decreasing = T )
wordcloud2( word_table2 )
wordcloud2( word_table2, color = 'random-light',
backgroundColor = 'black' )
wordcloud2( word_table2, fontFamily = '맑은고딕',
size = 1.2, color = 'random-light',
backgroundColor = 'black',
shape = 'star')
wordcloud2( word_table2, color = 'random-light',
backgroundColor = 'black' )
wordcloud2( word_table2, fontFamily = '맑은고딕',
size = 1.2, color = 'random-light',
backgroundColor = 'black',
shape = 'star')
wordcloud2( word_table2, size = 1.6,
color = rep_len(c('red','blue'), nrow(word_table2)))
wordcloud2( demoFreq, size = 1.6,
color = rep_len(c('red', 'blue'), nrow(word_table2)))
wordcloud2( word_table2, minRotation = -pi/6, maxRotation = -pi/6,
rotateRatio = 1 )
wordcloud2( demoFreq, minRotation = -pi/6, maxRotation = -pi/6,
rotateRatio = 1 )
#
# R 에서 웹문서 가져오기
#
# 웹에 있는 데이터를 가져오는 단계
#     요청: GET과 POST 방식
#     추출 및 저장
# 관련 R 패키지
#   XML, RCurl, httr, rvest, …
#
#
install.packages( "rvest" )
## 패키지 불러오기
library(rvest)
library(dplyr)
QUERY <- "제주" # 검색키워드
DATE  <- as.Date( as.character( 20191210 ), format = "%Y%m%d" ) # 검색시작날짜 & 검색종료날짜
DATE  <- format( DATE, "%Y.%m.%d" )
PAGE  <- 1
naver_url_1 <- "https://search.naver.com/search.naver?&where=news&query="
naver_url_2 <- "&pd=3&ds="
naver_url_3 <- "&de="
naver_url_4 <- "&start="
## 날짜 리스트 만들기
DATE_START <- as.Date( as.character( 20191210 ), format = "%Y%m%d" ) # 시작일자
DATE_END   <- as.Date( as.character( 20191210 ), format = "%Y%m%d" ) # 종료일자
DATE <- DATE_START:DATE_END
DATE <- as.Date( DATE, origin = "1970-01-01" )
## 게시물 번호 리스트 만들기
PAGE <- seq( from = 1, to = 41, by = 10 ) # 시작값과 종료값을 지정해줄 수 있습니다.
PAGE <- seq( from = 1, by = 10, length.out = 5) # 시작값과 원하는 갯수를 지정할 수도 있습니다.
## 네이버 검색결과 url 리스트에서 관련기사 url 리스트 만들기
news_url <- c()
news_date <-c()
for ( date_i in DATE ){
for ( page_i in PAGE ){
dt <- format( as.Date( date_i, origin = "1970-01-01" ), "%Y.%m.%d" )
naver_url <- paste0( naver_url_1, QUERY, naver_url_2, dt, naver_url_3, dt, naver_url_4, page_i )
html <- read_html( naver_url )
temp <- unique( html_nodes( html, '#main_pack' ) %>%     # id= 는 # 을 붙인다
html_nodes( css = '.news ' ) %>%         # class= 는 css= 를 붙인다
html_nodes( css = '.type01' ) %>%
html_nodes( 'a' )%>%
html_attr( 'href' ) )
news_url <- c( news_url, temp )
news_date <- c( news_date, rep( dt, length( temp ) ) )
}
print( dt ) # 진행상황을 알기 위함이니 속도가 느려지면 제외
}
NEWS0 <- as.data.frame( cbind( date = news_date, url = news_url, query = QUERY))
NEWS1 <- NEWS0[ which( grepl( "news.naver.com", NEWS0$url ) ), ]         # 네이버뉴스(news.naver.com)만 대상으로 한다
NEWS1 <- NEWS1[ which( !grepl( "sports.news.naver.com", NEWS1$url ) ), ] # 스포츠뉴스(sports.news.naver.com)는 제외한다
NEWS2 <- NEWS1[ !duplicated( NEWS1 ), ] # 중복된 링크 제거
NEWS2$news_title   <- ""
NEWS2$news_content <- ""
for ( i in 1:dim( NEWS2 )[ 1 ] ){
html <- read_html( as.character( NEWS2$url[ i ] ) )
temp_news_title   <- repair_encoding( html_text( html_nodes( html, '#articleTitle' ) ), from = 'utf-8' )
temp_news_content <- repair_encoding( html_text( html_nodes( html, '#articleBodyContents') ), from = 'utf-8' )
if ( length( temp_news_title ) > 0 ) {
NEWS2$news_title[ i ]   <- temp_news_title
NEWS2$news_content[i] <- temp_news_content
}
}
NEWS2$news_content <- gsub( "// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback()", "", NEWS2$news_content )
NEWS <- NEWS2 # 최종 결과 저장
NEWS
NEWS$news_content
NEWS
NEWS$news_content
# 워드클라우드
library( KoNLP )
useSejongDic()
word_data <- sapply( NEWS$news_content, extractNoun, USE.NAMES = F )
word_data
undata <- unlist( word_data )
undata
word_table <- table( undata )
word_table
undata2 <- undata[ nchar( undata ) >= 2 ]
undata2
word_table2 <- table( undata2 )
word_table2
sort( word_table2, decreasing = T )
library( wordcloud2 )
wordcloud2( word_table2, minRotation = -pi / 6, maxRotation = -pi / 6, rotateRatio = 1 )
setwd('D:/Rclass')
address_1 <- readLines( 'ex_10-1.txt', encoding = 'UTF-8' )
address_1
useSejongDic()
address_1_n <- sapply( address_1, extractNoun, USE.NAMES = F )
address_1_n
add1_word <- unlist(address_1_n)
add1_word_table <- table(add1_word)
add1_word_table
sort(add1_word_table, decreasing = T)
add_words <- c('경보', '위기', '책임', '가을', '정책', '대응', '대란', '조세')
buildDictionary( user_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T )
get_dictionary( 'user_dic' )
add1_words <- sapply( address_1, extractNoun, USE.NAMES = F )
add1_words
undata <- unlist(add1_words)
undata2 <- undata[ nchar( undata ) >= 2 ]
undata2
add1_table <- table(undata2)
sort(add1_table, decreasing = T)
wordcloud2( add1_table, fontFamily = '맑은고딕',
size = 1.2, color = 'random-light',
backgroundColor = 'black',
shape = 'star')
address_2 <- readLines( 'ex_10-2.txt', encoding = 'UTF-8' )
address_2_n <- sapply( address_2, extractNoun, USE.NAMES = F )
address_2_n
undata <- unlist(address_2_n)
undata2 <- undata[ nchar( undata ) >= 2 ]
add2_table <- table(undata2)
sort(add2_table, decreasing = T )
wordcloud2( add2_table, fontFamily = '맑은고딕',
size = 1.2, color = 'random-dark')
address_3 <- readLines( 'ex_10-3.txt', encoding = 'UTF-8' )
address_3_n <- sapply( address_3, extractNoun, USE.NAMES = F )
undata <- unlist(address_3_n)
undata2 <- undata[ nchar( undata ) >= 2 ]
add3_table <- table(undata2)
sort(add3_table, decreasing = T )
wordcloud( names( add3_table ), freq = wordcount, scale = c(5, 0.7),
min.freq = 1, random.order = F, rot.per = .1, colors = pal3 )
sort(add3_table, decreasing = T )
undata2
pal3 <- brewer.pal( 9, 'Blues' )[5:9]
wordcloud( names( add3_table ), freq = wordcount, scale = c(5, 0.7),
min.freq = 1, random.order = F, rot.per = .1, colors = pal3 )
add3_table <- table(undata2)
sort(add3_table, decreasing = T )
add3 <- sort(add3_table, decreasing = T )
add3
add3[1:30]
add3[1:50]
add3[1:40]
add3 <- sort(add3_table, decreasing = T )[1:40]
pal3 <- brewer.pal( 9, 'Blues' )[5:9]
wordcloud( names( add3_table ), freq = wordcount, scale = c(5, 0.7),
min.freq = 1, random.order = F, rot.per = .1, colors = pal3 )
add3 <- sort(add3_table, decreasing = T )[1:40]
pal3 <- brewer.pal( 9, 'Blues' )[5:9]
wordcloud( names( add3 ), freq = wordcount, scale = c(5, 0.7),
min.freq = 1, random.order = F, rot.per = .1, colors = pal3 )
stvjobs_stanford <- readLines( 'ex_10-4.txt' )
stvjobs_stanford
stvjobs_stanford <- readLines( 'ex_10-4.txt', encoding = 'UTF-8' )
stvjobs_stanford
stvjobs_word <- sapply( stvjobs_stanford, extractNoun, USE.NAMES = F )
undata <- unlist(stvjobs_word)
undata
table(undata)
sort(table(undata), decreasing = T)
undata2 <- undata[ nchar(undata) >= 2 ]
sort(table(undata2), decreasing = T)
undata2 <- gsub( '하지', '', undata2 )
undata2 <- gsub( '때문', '', undata2 )
undata2 <- gsub( '들이', '', undata2 )
wordcount <- table( undata2 )
wordcount
sort(wordcount, decreasing = T )
wordcloud2( wordcount, fontFamily = '맑은고딕',
size = 1.2, color = 'random-light',
backgroundColor = 'black')
undata2 <- gsub( '가지', '', undata2 )
wordcount <- table( undata2 )
sort(wordcount, decreasing = T )
wordcloud2( wordcount, fontFamily = '맑은고딕',
size = 1.2, color = 'random-light',
backgroundColor = 'black')
undata2 <- gsub( '개월', '', undata2 )
wordcount <- table( undata2 )
sort(wordcount, decreasing = T )
undata2 <- gsub( '하지', '', undata2 )
undata2 <- gsub( '때문', '', undata2 )
undata2 <- gsub( '들이', '', undata2 )
undata2 <- gsub( '가지', '', undata2 )
undata2 <- gsub( '개월', '', undata2 )
wordcount <- table( undata2 )
stvjobs_wordcount <- sort(wordcount, decreasing = T )[1:100]
wordcloud2( stvjobs_wordcount, fontFamily = '맑은고딕',
size = 1.2, color = 'random-light',
backgroundColor = 'black')
obama <- readlines('ex_10-5.txt', encoding = 'UTF-8')
obama
obama <- readlines('ex_10-5.txt', encoding = 'UTF-8')
obama
obama <- readLines('ex_10-5.txt', encoding = 'UTF-8')
obama
obama_undata <- unlist(obama_word)
sort(table(obama_undata))[1:30]
obama_word <- sapply(obama, extractNoun, USE.NAMES = F)
obama_undata <- unlist(obama_word)
sort(table(obama_undata))[1:30]
sort(table(obama_undata))[1:50]
sort(table(obama_undata))
sort(table(obama_undata), decreasing = T)
obama_undata2 <- obama_undata[ nchar(obama_undata) >= 2 ]
sort(table(obama_undata2), decreasing = T)
add_words <- c('캠페인', '민주주의')
buildDictionary( user_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T )
get_dictionary( 'user_dic' )
wordcloud2( obama_table, minRotation = -pi/6, maxRotation = -pi/6,
rotateRatio = 1 )
obama_table <- sort(table(obama_undata2), decreasing = T)
wordcloud2( obama_table, minRotation = -pi/6, maxRotation = -pi/6,
rotateRatio = 1 )
obama_undata2 <- gsub( '들이', '', obama_undata2 )
obama_undata2 <- gsub( '때문', '', obama_undata2 )
obama_undata2 <- gsub( '말하', '', obama_undata2 )
obama_undata2 <- gsub( '하지', '', obama_undata2 )
obama_table <- sort(table(obama_undata2), decreasing = T)
wordcloud2( obama_table, minRotation = -pi/6, maxRotation = -pi/6,
rotateRatio = 1 )
library(wordcloud)
library(wordcloud2)
library(KoNLP)
library(RColorBrewer)
library(dplyr)
library(ggplot2)
useSejongDic()
useNIADic()
useNIADic()
add1 <- readLines('ex_10-1.txt', encoding = 'UTF-8')
add1
add1_word <- sapply(add1, extractNoun, USE.NAMES = F)
sort(table(add1_word), decreasing = T)
sort(table(add1_word), decreasing = T)
add1_words <- unlist(add1_word)
sort(table(add1_words), decreasing = T)
add1 <- readLines('ex_10-1.txt', encoding = 'UTF-8')
add1_word <- sapply(add1, extractNoun, USE.NAMES = F)
add1_words <- unlist(add1_word)
add_words <- c('민생경제', '컨트롤타워', '비상시국', '위험신호',
'산업', 'IMF', '외환위기', '더불어민주당',
'성장전략', '분배', '공정', '가계부채', '소득양극화',
'소득불평등', '불공정', '법인세', '불공정', '인큐베이팅', '안보')
buildDictionary(user_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T)
get_dictionary('user_dic')
address1_word <- sapply(add1, extractNoun, USE.NAMES = F)
address1_words <- unlist(address1_word)
address1_words2 <- address1_words[nchar(address1_words) >=2 ]
sort(table(address1_words2), decreasing = T)
pal3 <- brewer.pal(9, 'Blues')[5:9]
wordcloud( names(add1_tabel), freq = add1_tabel, scale = c(6, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal3)
add1_table <- sort(table(address1_words2), decreasing = T)
pal3 <- brewer.pal(9, 'Blues')[5:9]
wordcloud( names(add1_table), freq = add1_table, scale = c(6, 0.7),
min.freq = 3, random.order = F, rot.per = .1, colors = pal3)
add2 <- readLines('ex_10-2.txt', encoding = 'UTF-8')
add2_word <- sapply(add2, extractNoun, USE.NAMES = F)
add2_words <- unlist(add2_word)
sort(table(add2_words), decreasing = T)
add_words <- c('댓글', '청탁', '김영란법' )
buildDictionary(user_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T)
get_dictionary('user_dic')
add_words <- c('댓글', '청탁', '김영란법' )
buildDictionary(user_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T)
get_dictionary('user_dic')
add2_word <- sapply(add2, extractNoun, USE.NAMES = F)
add2_words <- unlist(add2_word)
add2_words
add2_word <- sapply(add2, extractNoun, USE.NAMES = F)
add2_words <- unlist(add2_word)
add2_words_trim <- add2_words[ nchar(add2_words) >= 2 ]
add2_table <- sort(table(add2_words_trim), decreasing = T)
wordcloud2(add2_table, fontFamily = '맑은고딕', size = 1.2,
color = 'random-light', backgroundColor = 'black')
add3_word <- sapply(add3, extractNoun, USE.NAMES = F)
add3_words <- unlist(add3_word)
sort(table(add3_words), decreasing = T)
add3 <- readLines('ex_10-3.txt', encoding = 'UTF-8')
add3_word <- sapply(add3, extractNoun, USE.NAMES = F)
add3_words <- unlist(add3_word)
sort(table(add3_words), decreasing = T)
add3_words <- add3_words[ nchar(add3_words) >= 2 ]
sort(table(add3_words), decreasing = T)
add3_words <- gsub('때문', '', add3_words)
add3_words <- gsub('하지', '', add3_words)
add3_words <- gsub('하면', '', add3_words)
add3_words <- gsub('하자', '', add3_words)
add3_words <- gsub('해서', '', add3_words)
sort(table(add3_words), decreasing = T)
add3 <- readLines('ex_10-3.txt', encoding = 'UTF-8')
add3_word <- sapply(add3, extractNoun, USE.NAMES = F)
add3_words <- unlist(add3_word)
add_words <- c('국민의당', '비례대표',  '민주주의', '사드',
'전기요금', '한전', '사법개혁')
buildDictionary(user_dic = data.frame(add_words, rep('ncn', length(add_words))),
replace_usr_dic = T)
get_dictionary('user_dic')
add3_w <- sapply(add3, extractNoun, USE.NAMES = F)
add3_ws <- unlist(add3_w)
add3_ws <- gsub('때문', '', add3_ws)
add3_ws <- gsub('하지', '', add3_ws)
add3_ws <- gsub('하면', '', add3_ws)
add3_ws <- gsub('하자', '', add3_ws)
add3_ws <- gsub('해서', '', add3_ws)
add3_ws <- add3_ws[ nchar(add3_ws) >= 2 ]
sort(table(add3_ws), decreasing = T)
sort(table(add3_ws), decreasing = T)
add3_tb <- sort(table(add3_ws), decreasing = T)
wordcloud2( add3_tb, minRotation = -pi/6, maxRotation = -pi/6,
rotateRatio = 1 )
